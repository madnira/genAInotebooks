{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions :\n",
    "1. Local db object from milus exists; collection name - rag_collection\n",
    "2. the rag_collection has the embeddgings stored\n",
    "3. sentence transfoemer model is present in the local model path (for creating the query embeddigns)\n",
    "4. Gemini-1.0-pro model is used as a LLM with API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymilvus import MilvusClient \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\") # Make sure the API key is there in the environment variables\n",
    "print(API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the transformer models from local path to generate the embeddings and google gemini models LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the local model path and name \n",
    "\n",
    "model_name = \"all-mpnet-base-v2\"\n",
    "modelPath = f\"../model/{model_name}\"  \n",
    "# Load the SentenceTransformer embedding model fromthe local path\n",
    "embedding_model = SentenceTransformer(modelPath)\n",
    "\n",
    "# Loading the gemini model\n",
    "\n",
    "genai.configure(api_key=API_KEY)  #Configuring the genai object with the API key\n",
    "chat_model = genai.GenerativeModel('gemini-1.0-pro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the milvus client object and list the collection already present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAGchatclient = MilvusClient(\"../DB/RAG.db\") # ensure the local milvus db object is present in the path\n",
    "\n",
    "# list the collection\n",
    "res = RAGchatclient.list_collections() \n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function that will take the user query as input and return the top 2 citations and content for futher processing with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_index(question):\n",
    "    \"\"\"\n",
    "    The uesr question will be passed in tthis function and it will return the top 2 citations and\n",
    "    the 1st chunk retrieved from the index.\n",
    "    \"\"\"\n",
    "    q = [question]\n",
    "    query_vector = embedding_model.encode(q).tolist()\n",
    "\n",
    "    search_result = RAGchatclient.search(\n",
    "    collection_name=\"rag_collection\",  # target collection\n",
    "    data=query_vector,  # query vector from the user asked question\n",
    "    limit=3,  # returning the top 3 entities\n",
    "    output_fields=[\"Title\", \"Content\"],  # specifies fields to be returned\n",
    "    )\n",
    "\n",
    "    cit1 = search_result[0][0][\"entity\"][\"Title\"]\n",
    "    cit2 = search_result[0][1][\"entity\"][\"Title\"]\n",
    "    chunk1 = search_result[0][0][\"entity\"][\"Content\"]\n",
    "\n",
    "    return chunk1, cit1, cit2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function which will take the returned chunk from Vector DB Index and call the LLM to tailor the answer to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_llm(user_question):\n",
    "    returned_chunk, citation1, citation2 = query_index(user_question)\n",
    "\n",
    "    passage = returned_chunk\n",
    "    query = user_question\n",
    "    \n",
    "    prompt = (f\"\"\"Behave like a teacher who answers questions using text from the passage included below. \\\n",
    "          Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. \\\n",
    "          Try to give short answers for direct questions.\\\n",
    "          If the passage is irrelevant to the answer, you may ignore it.\n",
    "          QUESTION: '{query}'\n",
    "          PASSAGE: '{passage}'\n",
    "          ANSWER:\n",
    "        \"\"\")\n",
    "    \n",
    "    response = chat_model.generate_content(prompt)\n",
    "    \n",
    "    return response , citation1, citation2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"How can I assist : \")\n",
    "user_question = input()\n",
    "print(user_question)\n",
    "answer, citation1 , citation2 = chat_llm(user_question)\n",
    "\n",
    "print(f\"Ref1: {citation1} \\nRef2: {citation2}\")\n",
    "Markdown(answer.text) # answer is displayed as a markdown object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
